## Ranked 1st, "Attention-over-Attention Neural Networks for Reading Comprehension"
* [arXiv paper](https://arxiv.org/abs/1607.04423)
* [Code](https://github.com/OlavHN/attention-over-attention)

This is not the best setting. (which can achieve the 1st place performance)
With the same hyperparameters as reported in the paper, this implementation got an accuracy of 74.3% on both the validation and test set, compared with 73.1% and 74.4% reported by the author.
## Ranked 9th, "Bi-Directional Attention Flow for Machine Comprehension"
overall: https://allenai.github.io/bi-att-flow/
* [Code](https://github.com/allenai/bi-att-flow)
* [arXiv paper link](https://arxiv.org/pdf/1611.01603.pdf)
* [Demo](http://35.165.153.16:1995/)

## Etc.
* a little nifty tools that might help others to make the most out of SQuAD: [link](https://github.com/salmedina/SQuAD)
